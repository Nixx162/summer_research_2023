{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "from dataset import COCODataset\n",
    "from lightningmodule import LightningNet, GuidedLightningNet\n",
    "from utils import resize, predict, batch_loc_loss\n",
    "from model import Resnext50\n",
    "\n",
    "from captum.attr import Saliency\n",
    "from captum.attr import visualization as viz\\\n",
    "\n",
    "BATCH_SIZE = 128 if torch.cuda.is_available() else 64\n",
    "NUM_WORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = COCODataset('./pascalvoc', 'trainval')\n",
    "\n",
    "# print(os.path.join(top, file) for top, dir, file in os.walk(os.path.join('./pascalvoc', 'VOC' + 'trainval' + '_06-Nov-2007', 'VOCdevkit', 'VOC2007', 'JPEGImages')))\n",
    "\n",
    "# print(trainset.__getitem__(9))\n",
    "\n",
    "# print(images)\n",
    "# print(boxes)\n",
    "\n",
    "# new_image, new_boxes = resize(images, boxes, (224, 224))\n",
    "\n",
    "# print(new_image)\n",
    "# print(new_boxes)\n",
    "\n",
    "# tf = torchvision.transforms.Compose([\n",
    "#         torchvision.transforms.ToTensor(),\n",
    "#         torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "# new_image = tf(new_image)\n",
    "\n",
    "# print(new_image)\n",
    "\n",
    "# for top, dir, file in os.walk(os.path.join('./pascalvoc', 'VOC' + 'trainval' + '_06-Nov-2007', 'VOCdevkit', 'VOC2007', 'JPEGImages')):\n",
    "#     images = sorted([os.path.join(top, name) for name in file])\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "   # print(batch[0][2].size())\n",
    "\n",
    "   images = list()\n",
    "   boxes = list()\n",
    "   labels = list()\n",
    "\n",
    "   for b in batch:\n",
    "      images.append(b[0])\n",
    "      labels.append(b[1])\n",
    "      boxes.append(b[2])\n",
    "\n",
    "   images = torch.stack(images, dim=0)\n",
    "   # boxes = torch.stack(boxes, dim=0)\n",
    "   labels = torch.stack(labels, dim=0)\n",
    "\n",
    "   return images, labels, boxes \n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [0.9, 0.1], generator = torch.Generator().manual_seed(515))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=NUM_WORKERS, collate_fn = collate_fn)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=NUM_WORKERS, collate_fn = collate_fn)\n",
    "\n",
    "testset = COCODataset('pascalvoc', 'test')\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=NUM_WORKERS, collate_fn = collate_fn)\n",
    "\n",
    "batch = next(iter(trainloader))\n",
    "\n",
    "# print(x[1])\n",
    "# print(x[0])\n",
    "\n",
    "# inp = x[0][0]\n",
    "# lab = x[1][0]\n",
    "\n",
    "# print(inp)\n",
    "# print(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | Resnext50 | 23.0 M\n",
      "1 | loss  | BCELoss   | 0     \n",
      "------------------------------------\n",
      "23.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.0 M    Total params\n",
      "92.084    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:208: UserWarning: num_workers>0, persistent_workers=False, and strategy=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 126. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:05<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_class_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loc_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1112, in _run\n    results = self._run_stage()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1191, in _run_stage\n    self._run_train()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1214, in _run_train\n    self.fit_loop.run()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 267, in advance\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 213, in advance\n    batch_output = self.batch_loop.run(kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 88, in advance\n    outputs = self.optimizer_loop.run(optimizers, kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 202, in advance\n    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 249, in _run_optimization\n    self._optimizer_step(optimizer, opt_idx, kwargs.get(\"batch_idx\", 0), closure)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 370, in _optimizer_step\n    self.trainer._call_lightning_module_hook(\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1356, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/core/module.py\", line 1742, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py\", line 169, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 234, in optimizer_step\n    return self.precision_plugin.optimizer_step(\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 119, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 280, in wrapper\n    out = func(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 33, in _use_grad\n    ret = func(self, *args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/optim/sgd.py\", line 67, in step\n    loss = closure()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 105, in _wrap_closure\n    closure_result = closure()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 149, in __call__\n    self._result = self.closure(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 135, in closure\n    step_output = self._step_fn()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 419, in _training_step\n    training_step_output = self.trainer._call_strategy_hook(\"training_step\", *kwargs.values())\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1494, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 280, in training_step\n    return self.model(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n    output = self._run_ddp_forward(*inputs, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py\", line 98, in forward\n    output = self._forward_module.training_step(*inputs, **kwargs)\n  File \"/home/nicholas/summer_research_2023/guidedtraining/lightningmodule.py\", line 83, in training_step\n    attr = saliency.attribute(x, additional_forward_args=y)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/log/__init__.py\", line 42, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/attr/_core/saliency.py\", line 130, in attribute\n    gradients = self.gradient_func(\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/_utils/gradient.py\", line 112, in compute_gradients\n    outputs = _run_forward(forward_fn, inputs, target_ind, additional_forward_args)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/_utils/common.py\", line 482, in _run_forward\n    output = forward_func(\n  File \"/home/nicholas/summer_research_2023/guidedtraining/lightningmodule.py\", line 69, in model_wrapper\n    output = self.model(inputs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/summer_research_2023/guidedtraining/model.py\", line 20, in forward\n    return self.sigm(self.base_model(x))\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torchvision/models/resnet.py\", line 285, in forward\n    return self._forward_impl(x)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torchvision/models/resnet.py\", line 269, in _forward_impl\n    x = self.bn1(x)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n    return F.batch_norm(\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 15.74 GiB total capacity; 13.96 GiB already allocated; 33.50 MiB free; 14.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# print(toy(inp.unsqueeze(0)))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# print(predict(toy(inp.unsqueeze(0))))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs = 60, callbacks=[LearningRateMonitor(logging_interval=\"step\"), TQDMProgressBar(refresh_rate=10)])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# trainer.fit(resnet50, train_dataloaders = trainloader, val_dataloaders = valloader)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m guidedtrainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(devices\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m, max_epochs \u001b[39m=\u001b[39m \u001b[39m60\u001b[39m, callbacks\u001b[39m=\u001b[39m[LearningRateMonitor(logging_interval\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m), TQDMProgressBar(refresh_rate\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)])\n\u001b[0;32m---> 13\u001b[0m guidedtrainer\u001b[39m.\u001b[39;49mfit(guidedresnet50, train_dataloaders \u001b[39m=\u001b[39;49m trainloader, val_dataloaders \u001b[39m=\u001b[39;49m valloader)\n",
      "File \u001b[0;32m~/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:36\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mlauncher\u001b[39m.\u001b[39;49mlaunch(trainer_fn, \u001b[39m*\u001b[39;49margs, trainer\u001b[39m=\u001b[39;49mtrainer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:113\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     process_args \u001b[39m=\u001b[39m [trainer, function, args, kwargs, return_queue]\n\u001b[0;32m--> 113\u001b[0m mp\u001b[39m.\u001b[39;49mstart_processes(\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapping_function,\n\u001b[1;32m    115\u001b[0m     args\u001b[39m=\u001b[39;49mprocess_args,\n\u001b[1;32m    116\u001b[0m     nprocs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49mnum_processes,\n\u001b[1;32m    117\u001b[0m     start_method\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_start_method,\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    119\u001b[0m worker_output \u001b[39m=\u001b[39m return_queue\u001b[39m.\u001b[39mget()\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m trainer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    198\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[39mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[39m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1112, in _run\n    results = self._run_stage()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1191, in _run_stage\n    self._run_train()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1214, in _run_train\n    self.fit_loop.run()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 267, in advance\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 213, in advance\n    batch_output = self.batch_loop.run(kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 88, in advance\n    outputs = self.optimizer_loop.run(optimizers, kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 202, in advance\n    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 249, in _run_optimization\n    self._optimizer_step(optimizer, opt_idx, kwargs.get(\"batch_idx\", 0), closure)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 370, in _optimizer_step\n    self.trainer._call_lightning_module_hook(\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1356, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/core/module.py\", line 1742, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py\", line 169, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 234, in optimizer_step\n    return self.precision_plugin.optimizer_step(\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 119, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 280, in wrapper\n    out = func(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 33, in _use_grad\n    ret = func(self, *args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/optim/sgd.py\", line 67, in step\n    loss = closure()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 105, in _wrap_closure\n    closure_result = closure()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 149, in __call__\n    self._result = self.closure(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 135, in closure\n    step_output = self._step_fn()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 419, in _training_step\n    training_step_output = self.trainer._call_strategy_hook(\"training_step\", *kwargs.values())\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1494, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 280, in training_step\n    return self.model(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n    output = self._run_ddp_forward(*inputs, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py\", line 98, in forward\n    output = self._forward_module.training_step(*inputs, **kwargs)\n  File \"/home/nicholas/summer_research_2023/guidedtraining/lightningmodule.py\", line 83, in training_step\n    attr = saliency.attribute(x, additional_forward_args=y)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/log/__init__.py\", line 42, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/attr/_core/saliency.py\", line 130, in attribute\n    gradients = self.gradient_func(\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/_utils/gradient.py\", line 112, in compute_gradients\n    outputs = _run_forward(forward_fn, inputs, target_ind, additional_forward_args)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/_utils/common.py\", line 482, in _run_forward\n    output = forward_func(\n  File \"/home/nicholas/summer_research_2023/guidedtraining/lightningmodule.py\", line 69, in model_wrapper\n    output = self.model(inputs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/summer_research_2023/guidedtraining/model.py\", line 20, in forward\n    return self.sigm(self.base_model(x))\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torchvision/models/resnet.py\", line 285, in forward\n    return self._forward_impl(x)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torchvision/models/resnet.py\", line 269, in _forward_impl\n    x = self.bn1(x)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n    return F.batch_norm(\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 15.74 GiB total capacity; 13.96 GiB already allocated; 33.50 MiB free; 14.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "toy = Resnext50(20)\n",
    "resnet50 = LightningNet(Resnext50(20), 0.01)\n",
    "guidedresnet50 = GuidedLightningNet(Resnext50(20), 0.01)\n",
    "\n",
    "# print(toy(inp.unsqueeze(0)))\n",
    "# print(predict(toy(inp.unsqueeze(0))))\n",
    "\n",
    "# train model\n",
    "# trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs = 60, callbacks=[LearningRateMonitor(logging_interval=\"step\"), TQDMProgressBar(refresh_rate=10)])\n",
    "# trainer.fit(resnet50, train_dataloaders = trainloader, val_dataloaders = valloader)\n",
    "\n",
    "guidedtrainer = pl.Trainer(devices=4, accelerator=\"gpu\", max_epochs = 60, callbacks=[LearningRateMonitor(logging_interval=\"step\"), TQDMProgressBar(refresh_rate=10)])\n",
    "guidedtrainer.fit(guidedresnet50, train_dataloaders = trainloader, val_dataloaders = valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 224, 224])\n",
      "(128, 3, 224, 224)\n",
      "tensor(0.3203, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_model =  LightningNet.load_from_checkpoint(\"/home/nicholas/summer_research_2023/guidedtraining/lightning_logs/version_16/checkpoints/epoch=59-step=2160.ckpt\")\n",
    "\n",
    "# trainer.test(new_model, testloader)\n",
    "\n",
    "def model_wrapper(inputs, targets):\n",
    "    output = new_model(inputs)\n",
    "    # element-wise multiply outputs with one-hot encoded targets \n",
    "    # and compute sum of each row\n",
    "    # This sums the prediction for all markers which exist in the cell\n",
    "    return torch.sum(output * targets, dim=0)\n",
    "\n",
    "x, y = batch[0], batch[1]\n",
    "y_hat = new_model(x)\n",
    "loss = nn.BCELoss()\n",
    "classloss = loss(y_hat, y)\n",
    "saliency = Saliency(model_wrapper)\n",
    "attr = saliency.attribute(x, additional_forward_args=y)\n",
    "\n",
    "print(attr.shape)\n",
    "\n",
    "attr = attr.cpu().detach().numpy()\n",
    "\n",
    "print(attr.shape)\n",
    "\n",
    "locloss = batch_loc_loss(attr, batch[2])\n",
    "aveloss = (classloss + locloss) / 2\n",
    "\n",
    "print(aveloss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cifar10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
