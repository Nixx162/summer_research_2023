{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [45000, 5000], generator = torch.Generator().manual_seed(515))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=24)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=24)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv3 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels * 4, kernel_size = 1),\n",
    "                        nn.BatchNorm2d(out_channels * 4))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU())\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(1028, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            \n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * 4, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes * 4),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * 4\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitResNet(pl.LightningModule):\n",
    "    def __init__(self, model, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.nll_loss(y_hat, y)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = accuracy(preds, y, task='multiclass', num_classes=10)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.nll_loss(y_hat, y)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = accuracy(preds, y, task='multiclass', num_classes=10)\n",
    "        self.log(\"test_loss\", val_loss)\n",
    "        self.log(\"test_acc\", acc)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), self.lr, momentum=0.9)\n",
    "        return [optimizer]\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = torch.argmax(self(x), dim=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 23.5 M\n",
      "---------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.111    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:208: UserWarning: num_workers>0, persistent_workers=False, and strategy=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 2 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1112, in _run\n    results = self._run_stage()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1191, in _run_stage\n    self._run_train()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1204, in _run_train\n    self._run_sanity_check()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1276, in _run_sanity_check\n    val_loop.run()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 152, in advance\n    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 137, in advance\n    output = self._evaluation_step(**kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 234, in _evaluation_step\n    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1494, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 288, in validation_step\n    return self.model(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n    output = self._run_ddp_forward(*inputs, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py\", line 110, in forward\n    return self._forward_module.validation_step(*inputs, **kwargs)\n  File \"/tmp/ipykernel_1612902/3510837880.py\", line 20, in validation_step\n    y_hat = self(x)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1612902/3510837880.py\", line 9, in forward\n    out = self.model(x)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1612902/845862331.py\", line 37, in forward\n    x = self.layer0(x)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1612902/1303929654.py\", line 26, in forward\n    out += residual\nRuntimeError: The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m, max_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, default_root_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./basicnn/checkpoint\u001b[39m\u001b[39m\"\u001b[39m, callbacks\u001b[39m=\u001b[39m[LearningRateMonitor(logging_interval\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m), TQDMProgressBar(refresh_rate\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)])\n\u001b[0;32m----> 5\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mbasicnn, train_dataloaders \u001b[39m=\u001b[39;49m trainloader, val_dataloaders \u001b[39m=\u001b[39;49m valloader)\n",
      "File \u001b[0;32m~/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:36\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mlauncher\u001b[39m.\u001b[39;49mlaunch(trainer_fn, \u001b[39m*\u001b[39;49margs, trainer\u001b[39m=\u001b[39;49mtrainer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:113\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     process_args \u001b[39m=\u001b[39m [trainer, function, args, kwargs, return_queue]\n\u001b[0;32m--> 113\u001b[0m mp\u001b[39m.\u001b[39;49mstart_processes(\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapping_function,\n\u001b[1;32m    115\u001b[0m     args\u001b[39m=\u001b[39;49mprocess_args,\n\u001b[1;32m    116\u001b[0m     nprocs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49mnum_processes,\n\u001b[1;32m    117\u001b[0m     start_method\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_start_method,\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    119\u001b[0m worker_output \u001b[39m=\u001b[39m return_queue\u001b[39m.\u001b[39mget()\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m trainer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    198\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[39mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[39m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 2 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1112, in _run\n    results = self._run_stage()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1191, in _run_stage\n    self._run_train()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1204, in _run_train\n    self._run_sanity_check()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1276, in _run_sanity_check\n    val_loop.run()\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 152, in advance\n    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 137, in advance\n    output = self._evaluation_step(**kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 234, in _evaluation_step\n    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1494, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 288, in validation_step\n    return self.model(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n    output = self._run_ddp_forward(*inputs, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py\", line 110, in forward\n    return self._forward_module.validation_step(*inputs, **kwargs)\n  File \"/tmp/ipykernel_1612902/3510837880.py\", line 20, in validation_step\n    y_hat = self(x)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1612902/3510837880.py\", line 9, in forward\n    out = self.model(x)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1612902/845862331.py\", line 37, in forward\n    x = self.layer0(x)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/home/nicholas/anaconda3/envs/cifar10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1612902/1303929654.py\", line 26, in forward\n    out += residual\nRuntimeError: The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "basicnn = LitResNet(ResNet(BottleneckBlock, [3, 4, 6, 3]), 0.01)\n",
    "\n",
    "# train model\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", max_epochs = 10, default_root_dir=\"./basicnn/checkpoint\", callbacks=[LearningRateMonitor(logging_interval=\"step\"), TQDMProgressBar(refresh_rate=10)])\n",
    "trainer.fit(model=basicnn, train_dataloaders = trainloader, val_dataloaders = valloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cifar10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
